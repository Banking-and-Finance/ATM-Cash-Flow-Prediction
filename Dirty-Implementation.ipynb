{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, BayesianRidge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"AggregatedData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Convert Weekday column to Uppercase because of format mismatch </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Weekday'] = data['Weekday'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11589 entries, 0 to 11588\n",
      "Data columns (total 12 columns):\n",
      "ATM Name                        11589 non-null object\n",
      "Transaction Date                11589 non-null object\n",
      "No Of Withdrawals               11589 non-null int64\n",
      "No Of XYZ Card Withdrawals      11589 non-null int64\n",
      "No Of Other Card Withdrawals    11589 non-null int64\n",
      "Total amount Withdrawn          11589 non-null int64\n",
      "Amount withdrawn XYZ Card       11589 non-null int64\n",
      "Amount withdrawn Other Card     11589 non-null int64\n",
      "Weekday                         11589 non-null object\n",
      "Festival Religion               11589 non-null object\n",
      "Working Day                     11589 non-null object\n",
      "Holiday Sequence                11589 non-null object\n",
      "dtypes: int64(6), object(6)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Convert Transaction Date column to Date Time object </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "data['Transaction Date'] = pd.to_datetime(data['Transaction Date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATM Name</th>\n",
       "      <th>Transaction Date</th>\n",
       "      <th>No Of Withdrawals</th>\n",
       "      <th>No Of XYZ Card Withdrawals</th>\n",
       "      <th>No Of Other Card Withdrawals</th>\n",
       "      <th>Total amount Withdrawn</th>\n",
       "      <th>Amount withdrawn XYZ Card</th>\n",
       "      <th>Amount withdrawn Other Card</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Festival Religion</th>\n",
       "      <th>Working Day</th>\n",
       "      <th>Holiday Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Street ATM</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>123800</td>\n",
       "      <td>41700</td>\n",
       "      <td>82100</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mount Road ATM</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>253</td>\n",
       "      <td>67</td>\n",
       "      <td>186</td>\n",
       "      <td>767900</td>\n",
       "      <td>270900</td>\n",
       "      <td>497000</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Airport ATM</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>98</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>503400</td>\n",
       "      <td>347700</td>\n",
       "      <td>155700</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KK Nagar ATM</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>265</td>\n",
       "      <td>159</td>\n",
       "      <td>106</td>\n",
       "      <td>945300</td>\n",
       "      <td>532600</td>\n",
       "      <td>412700</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christ College ATM</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>74</td>\n",
       "      <td>25</td>\n",
       "      <td>49</td>\n",
       "      <td>287700</td>\n",
       "      <td>148200</td>\n",
       "      <td>139500</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ATM Name Transaction Date  No Of Withdrawals  \\\n",
       "0      Big Street ATM       2011-01-01                 50   \n",
       "1      Mount Road ATM       2011-01-01                253   \n",
       "2         Airport ATM       2011-01-01                 98   \n",
       "3        KK Nagar ATM       2011-01-01                265   \n",
       "4  Christ College ATM       2011-01-01                 74   \n",
       "\n",
       "   No Of XYZ Card Withdrawals  No Of Other Card Withdrawals  \\\n",
       "0                          20                            30   \n",
       "1                          67                           186   \n",
       "2                          56                            42   \n",
       "3                         159                           106   \n",
       "4                          25                            49   \n",
       "\n",
       "   Total amount Withdrawn  Amount withdrawn XYZ Card  \\\n",
       "0                  123800                      41700   \n",
       "1                  767900                     270900   \n",
       "2                  503400                     347700   \n",
       "3                  945300                     532600   \n",
       "4                  287700                     148200   \n",
       "\n",
       "   Amount withdrawn Other Card   Weekday Festival Religion Working Day  \\\n",
       "0                        82100  SATURDAY                 H           H   \n",
       "1                       497000  SATURDAY                 C           H   \n",
       "2                       155700  SATURDAY                 C           H   \n",
       "3                       412700  SATURDAY                 C           H   \n",
       "4                       139500  SATURDAY                 C           H   \n",
       "\n",
       "  Holiday Sequence  \n",
       "0              WHH  \n",
       "1              WHH  \n",
       "2              WHH  \n",
       "3              WHH  \n",
       "4              WHH  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing the XYZ and Other Card Withdrawn Amounts columns too because I think they will end up biasing the model too much because the Total Amount Withdrawn column is just the sum of these two columns </h3>\n",
    "<h4> Actually the number of withdrawals should be removed too, because in a real-time scenario, we cannot really have that value but I'm keeping it in for now </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.drop(['Transaction Date', 'Amount withdrawn Other Card',\n",
    "               'Amount withdrawn XYZ Card', 'No Of Withdrawals', 'No Of XYZ Card Withdrawals',\n",
    "       'No Of Other Card Withdrawals'], axis = 1)\n",
    "\n",
    "# y = data['Total amount Withdrawn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Converting Transaction Date to separate columns, otherwise model won't accept it </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Month'] = data['Transaction Date'].dt.month\n",
    "new_data['Day'] = data['Transaction Date'].dt.day\n",
    "new_data['Year'] = data['Transaction Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ATM Name', 'Total amount Withdrawn', 'Weekday', 'Festival Religion',\n",
       "       'Working Day', 'Holiday Sequence', 'Month', 'Day', 'Year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Converting Categorical Columns to Boolean Columns using pd.get_dummies() </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_list = ['ATM Name', 'Weekday', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'Month', 'Day', 'Year']\n",
    "numeric_features_list = []\n",
    "testing_summary = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categorical columns to Dummy Data (One-Hot Encoding I think)\n",
    "# drop_first = True to avoid the first column of each dummy column's result\n",
    "# So if column = Gender and has two unique values Male and Female, get_dummies on this column creates two new columns\n",
    "# male and female, if person male that column is 1 and the other is 0 and same for female column, but we only just need\n",
    "# one of these columns, male or female, if male is 0 it guarantees person is female, for that reason drop_first=True\n",
    "def convert_categorical_to_numerical(data, column_list):\n",
    "    if 'ATM Name' in column_list:\n",
    "        column_list.remove('ATM Name')\n",
    "        temp_data = pd.get_dummies(data, columns=column_list , drop_first=True)\n",
    "    \n",
    "        # Do drop_first for all columns, except for ATM Name, because it becomes annoying later on\n",
    "        return pd.get_dummies(temp_data, columns=['ATM Name'])\n",
    "    else:\n",
    "        return pd.get_dummies(data, columns=column_list, drop_first=True)\n",
    "\n",
    "\n",
    "temp_numeric_data = convert_categorical_to_numerical(new_data, categorical_features_list)\n",
    "numeric_data = temp_numeric_data.drop('ATM Name_KK Nagar ATM', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total amount Withdrawn</th>\n",
       "      <th>Weekday_MONDAY</th>\n",
       "      <th>Weekday_SATURDAY</th>\n",
       "      <th>Weekday_SUNDAY</th>\n",
       "      <th>Weekday_THURSDAY</th>\n",
       "      <th>Weekday_TUESDAY</th>\n",
       "      <th>Weekday_WEDNESDAY</th>\n",
       "      <th>Festival Religion_H</th>\n",
       "      <th>Festival Religion_M</th>\n",
       "      <th>Festival Religion_N</th>\n",
       "      <th>...</th>\n",
       "      <th>Year_2012</th>\n",
       "      <th>Year_2013</th>\n",
       "      <th>Year_2014</th>\n",
       "      <th>Year_2015</th>\n",
       "      <th>Year_2016</th>\n",
       "      <th>Year_2017</th>\n",
       "      <th>ATM Name_Airport ATM</th>\n",
       "      <th>ATM Name_Big Street ATM</th>\n",
       "      <th>ATM Name_Christ College ATM</th>\n",
       "      <th>ATM Name_Mount Road ATM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>767900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>503400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>945300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>287700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total amount Withdrawn  Weekday_MONDAY  Weekday_SATURDAY  Weekday_SUNDAY  \\\n",
       "0                  123800               0                 1               0   \n",
       "1                  767900               0                 1               0   \n",
       "2                  503400               0                 1               0   \n",
       "3                  945300               0                 1               0   \n",
       "4                  287700               0                 1               0   \n",
       "\n",
       "   Weekday_THURSDAY  Weekday_TUESDAY  Weekday_WEDNESDAY  Festival Religion_H  \\\n",
       "0                 0                0                  0                    1   \n",
       "1                 0                0                  0                    0   \n",
       "2                 0                0                  0                    0   \n",
       "3                 0                0                  0                    0   \n",
       "4                 0                0                  0                    0   \n",
       "\n",
       "   Festival Religion_M  Festival Religion_N  ...  Year_2012  Year_2013  \\\n",
       "0                    0                    0  ...          0          0   \n",
       "1                    0                    0  ...          0          0   \n",
       "2                    0                    0  ...          0          0   \n",
       "3                    0                    0  ...          0          0   \n",
       "4                    0                    0  ...          0          0   \n",
       "\n",
       "   Year_2014  Year_2015  Year_2016  Year_2017  ATM Name_Airport ATM  \\\n",
       "0          0          0          0          0                     0   \n",
       "1          0          0          0          0                     0   \n",
       "2          0          0          0          0                     1   \n",
       "3          0          0          0          0                     0   \n",
       "4          0          0          0          0                     0   \n",
       "\n",
       "   ATM Name_Big Street ATM  ATM Name_Christ College ATM  \\\n",
       "0                        1                            0   \n",
       "1                        0                            0   \n",
       "2                        0                            0   \n",
       "3                        0                            0   \n",
       "4                        0                            1   \n",
       "\n",
       "   ATM Name_Mount Road ATM  \n",
       "0                        0  \n",
       "1                        1  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train Test Split followed by Scaling all Columns </h3>\n",
    "<h3> Since all columns are non-numeric, we don't really need scaling, so I commented it out for now </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = numeric_data.drop('Total amount Withdrawn', axis=1)\n",
    "y = numeric_data['Total amount Withdrawn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "# Note that we fit the scaler on X_train only and not X, otherwise test data will get biased on means and std of test data\n",
    "# Instead it should use train data mean and std\n",
    "# scaler = StandardScaler().fit(X_train)\n",
    "# scaled_X_train = pd.DataFrame(data=scaler.transform(X_train), columns=X.columns)\n",
    "# scaled_X_test = pd.DataFrame(data=scaler.transform(X_test), columns=X.columns)\n",
    "\n",
    "# scaled_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> KNN Implementation & Error Rate Visualization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1 Model Trained and Tested\n",
      "k=2 Model Trained and Tested\n",
      "k=3 Model Trained and Tested\n",
      "k=4 Model Trained and Tested\n",
      "k=5 Model Trained and Tested\n",
      "k=6 Model Trained and Tested\n",
      "k=7 Model Trained and Tested\n",
      "k=8 Model Trained and Tested\n",
      "k=9 Model Trained and Tested\n",
      "k=10 Model Trained and Tested\n",
      "k=11 Model Trained and Tested\n",
      "k=12 Model Trained and Tested\n",
      "k=13 Model Trained and Tested\n",
      "k=14 Model Trained and Tested\n",
      "k=15 Model Trained and Tested\n",
      "k=16 Model Trained and Tested\n",
      "k=17 Model Trained and Tested\n",
      "k=18 Model Trained and Tested\n",
      "k=19 Model Trained and Tested\n",
      "k=20 Model Trained and Tested\n",
      "k=21 Model Trained and Tested\n",
      "k=22 Model Trained and Tested\n",
      "k=23 Model Trained and Tested\n",
      "k=24 Model Trained and Tested\n",
      "k=25 Model Trained and Tested\n",
      "k=26 Model Trained and Tested\n",
      "k=27 Model Trained and Tested\n",
      "k=28 Model Trained and Tested\n",
      "k=29 Model Trained and Tested\n",
      "k=30 Model Trained and Tested\n",
      "k=31 Model Trained and Tested\n",
      "k=32 Model Trained and Tested\n",
      "k=33 Model Trained and Tested\n",
      "k=34 Model Trained and Tested\n",
      "k=35 Model Trained and Tested\n",
      "k=36 Model Trained and Tested\n",
      "k=37 Model Trained and Tested\n",
      "k=38 Model Trained and Tested\n",
      "k=39 Model Trained and Tested\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "error_rate = []\n",
    "for k in range(1, 40):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, n_jobs=-1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions = knn.predict(X_test)\n",
    "    error_rate.append(np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "    print(\"k={} Model Trained and Tested\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a7aeb86af0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40), error_rate, linestyle='--', marker='o', markerfacecolor='red', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 36\n",
      "Best RMSE: 263821.32380546525\n"
     ]
    }
   ],
   "source": [
    "print(\"Best K:\", error_rate.index(min(error_rate)))\n",
    "print(\"Best RMSE:\", min(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Best RMSE: 243180.0491744677 </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running Each Regression Model to get summary of all RMSEs </h2>\n",
    "<h3> Models tested </h3>\n",
    "<ul>\n",
    "    <li> KNN </li>\n",
    "    <li> Linear Regression </li>\n",
    "    <li> Ridge </li>\n",
    "    <li> Elasticnet </li>\n",
    "    <li> Lasso </li>\n",
    "    <li> Bayesian Ridge </li>\n",
    "    <li> Support Vector Regression </li>\n",
    "    <li> Random Forest Regression </li>\n",
    "    <li> Decision Tree Regression </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'K-Nearest Neighbours': KNeighborsRegressor(n_neighbors=37, n_jobs=-1), 'Vanilla Linear Regression': LinearRegression(), \n",
    "          'Ridge Linear Regression': Ridge(), 'Elasticnet Linear Regression': ElasticNet(), \n",
    "          'Lasso Linear Regression': Lasso(max_iter=1100), 'Bayesian Ridge Linear Regression': BayesianRidge(), \n",
    "          'Support Vector Regression': SVR(), 'Random Forest Regression': RandomForestRegressor(), \n",
    "          'Decision Tree Regression': DecisionTreeRegressor()}\n",
    "\n",
    "atm_names = new_data['ATM Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_model_list(models, X_train, y_train, X_test, y_test):\n",
    "    print(\"RMSEs for each model\")\n",
    "    testing_scores = dict()\n",
    "    for model_name in models:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        model_predictions = model.predict(X_test)\n",
    "        model_rmse = np.sqrt(mean_squared_error(y_test, model_predictions))\n",
    "        model_testing_score = model.score(X_test, y_test)\n",
    "        model_training_score = model.score(X_train, y_train)\n",
    "        print(\"For\", model_name)\n",
    "        print(\"\\tTesting RMSE = {}\".format(model_rmse))\n",
    "        print(\"\\tTraining Score =\", model_training_score)\n",
    "        print(\"\\tTesting Score =\", model_testing_score)\n",
    "        \n",
    "        # Storing the results in a dictionary, so stuff can be easily compared later\n",
    "        testing_scores[model_name] = dict()\n",
    "        testing_scores[model_name]['Training Score'] = model_training_score\n",
    "        testing_scores[model_name]['Testing Score'] = model_testing_score\n",
    "        testing_scores[model_name]['RMSE'] = model_rmse\n",
    "    \n",
    "    return testing_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 263821.32380546525\n",
      "\tTraining Score = 0.376755664209415\n",
      "\tTesting Score = 0.34976990282315645\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 251419.8192235889\n",
      "\tTraining Score = 0.4013361571445507\n",
      "\tTesting Score = 0.4094641102628562\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 251395.20976856566\n",
      "\tTraining Score = 0.401303262994798\n",
      "\tTesting Score = 0.40957971018028033\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 309411.6362690149\n",
      "\tTraining Score = 0.10900893738529349\n",
      "\tTesting Score = 0.10562312336992907\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 251416.0051469024\n",
      "\tTraining Score = 0.40133597818305683\n",
      "\tTesting Score = 0.40948202716449345\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 251414.41701770766\n",
      "\tTraining Score = 0.4009356351971536\n",
      "\tTesting Score = 0.4094894874363259\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 330060.3547257065\n",
      "\tTraining Score = -0.02381723071492714\n",
      "\tTesting Score = -0.017733339289077277\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 207957.12120150332\n",
      "\tTraining Score = 0.9384526250666212\n",
      "\tTesting Score = 0.5959874361254779\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 255828.3221501289\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = 0.38857312728564913\n"
     ]
    }
   ],
   "source": [
    "testing_summary['All ATMs Trained with Train Test Split'] = compute_rmse_model_list(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The scores and RMSEs computed by the above function are stored in the dictionary in the following format </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> After observing the testing and training scores, I think its clear that Decision Tree overfits, but I dont think that was ever that high on our sought after models list, so that's good, other than that everything is not ideal but atleast no overfitting </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Using only 2017 data as test data and everything else as training data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total amount Withdrawn</th>\n",
       "      <th>Weekday_MONDAY</th>\n",
       "      <th>Weekday_SATURDAY</th>\n",
       "      <th>Weekday_SUNDAY</th>\n",
       "      <th>Weekday_THURSDAY</th>\n",
       "      <th>Weekday_TUESDAY</th>\n",
       "      <th>Weekday_WEDNESDAY</th>\n",
       "      <th>Festival Religion_H</th>\n",
       "      <th>Festival Religion_M</th>\n",
       "      <th>Festival Religion_N</th>\n",
       "      <th>...</th>\n",
       "      <th>Year_2012</th>\n",
       "      <th>Year_2013</th>\n",
       "      <th>Year_2014</th>\n",
       "      <th>Year_2015</th>\n",
       "      <th>Year_2016</th>\n",
       "      <th>Year_2017</th>\n",
       "      <th>ATM Name_Airport ATM</th>\n",
       "      <th>ATM Name_Big Street ATM</th>\n",
       "      <th>ATM Name_Christ College ATM</th>\n",
       "      <th>ATM Name_Mount Road ATM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>767900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>503400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>945300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>287700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11584</th>\n",
       "      <td>468800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11585</th>\n",
       "      <td>305100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>709900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11587</th>\n",
       "      <td>408700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11588</th>\n",
       "      <td>700400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11589 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total amount Withdrawn  Weekday_MONDAY  Weekday_SATURDAY  \\\n",
       "0                      123800               0                 1   \n",
       "1                      767900               0                 1   \n",
       "2                      503400               0                 1   \n",
       "3                      945300               0                 1   \n",
       "4                      287700               0                 1   \n",
       "...                       ...             ...               ...   \n",
       "11584                  468800               0                 0   \n",
       "11585                  305100               0                 0   \n",
       "11586                  709900               0                 0   \n",
       "11587                  408700               0                 0   \n",
       "11588                  700400               0                 0   \n",
       "\n",
       "       Weekday_SUNDAY  Weekday_THURSDAY  Weekday_TUESDAY  Weekday_WEDNESDAY  \\\n",
       "0                   0                 0                0                  0   \n",
       "1                   0                 0                0                  0   \n",
       "2                   0                 0                0                  0   \n",
       "3                   0                 0                0                  0   \n",
       "4                   0                 0                0                  0   \n",
       "...               ...               ...              ...                ...   \n",
       "11584               0                 0                0                  0   \n",
       "11585               0                 0                0                  0   \n",
       "11586               0                 0                0                  0   \n",
       "11587               0                 0                0                  0   \n",
       "11588               0                 0                0                  0   \n",
       "\n",
       "       Festival Religion_H  Festival Religion_M  Festival Religion_N  ...  \\\n",
       "0                        1                    0                    0  ...   \n",
       "1                        0                    0                    0  ...   \n",
       "2                        0                    0                    0  ...   \n",
       "3                        0                    0                    0  ...   \n",
       "4                        0                    0                    0  ...   \n",
       "...                    ...                  ...                  ...  ...   \n",
       "11584                    1                    0                    0  ...   \n",
       "11585                    1                    0                    0  ...   \n",
       "11586                    1                    0                    0  ...   \n",
       "11587                    1                    0                    0  ...   \n",
       "11588                    1                    0                    0  ...   \n",
       "\n",
       "       Year_2012  Year_2013  Year_2014  Year_2015  Year_2016  Year_2017  \\\n",
       "0              0          0          0          0          0          0   \n",
       "1              0          0          0          0          0          0   \n",
       "2              0          0          0          0          0          0   \n",
       "3              0          0          0          0          0          0   \n",
       "4              0          0          0          0          0          0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11584          0          0          0          0          0          1   \n",
       "11585          0          0          0          0          0          1   \n",
       "11586          0          0          0          0          0          1   \n",
       "11587          0          0          0          0          0          1   \n",
       "11588          0          0          0          0          0          1   \n",
       "\n",
       "       ATM Name_Airport ATM  ATM Name_Big Street ATM  \\\n",
       "0                         0                        1   \n",
       "1                         0                        0   \n",
       "2                         1                        0   \n",
       "3                         0                        0   \n",
       "4                         0                        0   \n",
       "...                     ...                      ...   \n",
       "11584                     0                        1   \n",
       "11585                     0                        0   \n",
       "11586                     1                        0   \n",
       "11587                     0                        0   \n",
       "11588                     0                        0   \n",
       "\n",
       "       ATM Name_Christ College ATM  ATM Name_Mount Road ATM  \n",
       "0                                0                        0  \n",
       "1                                0                        1  \n",
       "2                                0                        0  \n",
       "3                                0                        0  \n",
       "4                                1                        0  \n",
       "...                            ...                      ...  \n",
       "11584                            0                        0  \n",
       "11585                            0                        1  \n",
       "11586                            0                        0  \n",
       "11587                            0                        0  \n",
       "11588                            1                        0  \n",
       "\n",
       "[11589 rows x 70 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10434\n",
      "1155\n"
     ]
    }
   ],
   "source": [
    "train_data = numeric_data[numeric_data['Year_2017'] == 0]\n",
    "test_data = numeric_data[numeric_data['Year_2017'] == 1]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "X_train = train_data.drop('Total amount Withdrawn', axis=1)\n",
    "y_train = train_data['Total amount Withdrawn']\n",
    "\n",
    "X_test = test_data.drop('Total amount Withdrawn', axis=1)\n",
    "y_test = test_data['Total amount Withdrawn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 349507.030798516\n",
      "\tTraining Score = 0.38667187416152216\n",
      "\tTesting Score = -0.07024480833352564\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 351236.18526535534\n",
      "\tTraining Score = 0.4535210215154458\n",
      "\tTesting Score = -0.080860883614569\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 350822.0331627114\n",
      "\tTraining Score = 0.453491696908386\n",
      "\tTesting Score = -0.0783134416727651\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 341559.4825601102\n",
      "\tTraining Score = 0.11927880563347205\n",
      "\tTesting Score = -0.02212495241294432\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 351203.8417642888\n",
      "\tTraining Score = 0.4535207159268131\n",
      "\tTesting Score = -0.08066183114206793\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 350009.66670241096\n",
      "\tTraining Score = 0.45328800801809777\n",
      "\tTesting Score = -0.0733253202260633\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 339521.12487241626\n",
      "\tTraining Score = -0.025157973486803265\n",
      "\tTesting Score = -0.009961686343598508\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 401710.74721486005\n",
      "\tTraining Score = 0.9437887376038445\n",
      "\tTesting Score = -0.4138332736815502\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 416128.8952306574\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -0.5171448371948302\n"
     ]
    }
   ],
   "source": [
    "testing_summary['All ATMs Trained with 2017 Test Data'] = compute_rmse_model_list(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Random Forest and Decision Tree overfit here but everything else is trash for both training and testing, idk what can be done about this </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Finding out the total amount withdrawn per month per year </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATM Name</th>\n",
       "      <th>Total amount Withdrawn</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Festival Religion</th>\n",
       "      <th>Working Day</th>\n",
       "      <th>Holiday Sequence</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Street ATM</td>\n",
       "      <td>123800</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mount Road ATM</td>\n",
       "      <td>767900</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Airport ATM</td>\n",
       "      <td>503400</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KK Nagar ATM</td>\n",
       "      <td>945300</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christ College ATM</td>\n",
       "      <td>287700</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11584</th>\n",
       "      <td>Big Street ATM</td>\n",
       "      <td>468800</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11585</th>\n",
       "      <td>Mount Road ATM</td>\n",
       "      <td>305100</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>Airport ATM</td>\n",
       "      <td>709900</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11587</th>\n",
       "      <td>KK Nagar ATM</td>\n",
       "      <td>408700</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11588</th>\n",
       "      <td>Christ College ATM</td>\n",
       "      <td>700400</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11589 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ATM Name  Total amount Withdrawn   Weekday Festival Religion  \\\n",
       "0          Big Street ATM                  123800  SATURDAY                 H   \n",
       "1          Mount Road ATM                  767900  SATURDAY                 C   \n",
       "2             Airport ATM                  503400  SATURDAY                 C   \n",
       "3            KK Nagar ATM                  945300  SATURDAY                 C   \n",
       "4      Christ College ATM                  287700  SATURDAY                 C   \n",
       "...                   ...                     ...       ...               ...   \n",
       "11584      Big Street ATM                  468800    FRIDAY                 H   \n",
       "11585      Mount Road ATM                  305100    FRIDAY                 H   \n",
       "11586         Airport ATM                  709900    FRIDAY                 H   \n",
       "11587        KK Nagar ATM                  408700    FRIDAY                 H   \n",
       "11588  Christ College ATM                  700400    FRIDAY                 H   \n",
       "\n",
       "      Working Day Holiday Sequence  Month  Day  Year  \n",
       "0               H              WHH      1    1  2011  \n",
       "1               H              WHH      1    1  2011  \n",
       "2               H              WHH      1    1  2011  \n",
       "3               H              WHH      1    1  2011  \n",
       "4               H              WHH      1    1  2011  \n",
       "...           ...              ...    ...  ...   ...  \n",
       "11584           H              WHH      9   29  2017  \n",
       "11585           H              WHH      9   29  2017  \n",
       "11586           H              WHH      9   29  2017  \n",
       "11587           H              WHH      9   29  2017  \n",
       "11588           H              WHH      9   29  2017  \n",
       "\n",
       "[11589 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes like 15-20 seconds to compute\n",
    "total_amt_per_mon_per_yr = {}\n",
    "for year in new_data['Year'].unique():\n",
    "    for month in new_data['Month'].unique():\n",
    "        total_amt_per_mon_per_yr[(year, month)] = sum(new_data[new_data.apply(lambda x: x['Year'] == year\n",
    "                            and x['Month'] == month, axis=1)]['Total amount Withdrawn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> total_amt_per_mon_per_yr is basically a dictionary whose key is a tuple (year, month) and its value contains the total amount withdrawn in the month of the year specified in the key, so (2011, 1): 65360200, means that 65360200 was withdrawn in total in the month of January 2011 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Uncomment this if you want to see what the dictionary is like but its a big list and takes up space so I commented it out\n",
    "# pprint(total_amt_per_mon_per_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Amount Withdrawn: 6053002800\n",
      "Average Amount Withdrawn: 72059557.14285715\n"
     ]
    }
   ],
   "source": [
    "total_amount_withdrawn = sum(total_amt_per_mon_per_yr.values())\n",
    "avg_amount_withdrawn = sum(total_amt_per_mon_per_yr.values()) / len(total_amt_per_mon_per_yr.values())\n",
    "print(\"Total Amount Withdrawn:\", total_amount_withdrawn)\n",
    "print(\"Average Amount Withdrawn:\", avg_amount_withdrawn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Figuring out Outlier Months </h3>\n",
    "<p> So my logic is that I take the total amount withdrawn in each month (of each year ofc) and then check its difference from the average amount withdrawn per month per year, and there's a tolerance percent variable (currently 0.7), so if the difference (positive or negative) is greater than 70% of the avg_amount_withdrawn, then that month of that year is an outlier </p>\n",
    "<p> I think something similar can be done with days in a month as well </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2016, 11)\n",
      "(2016, 12)\n",
      "(2017, 10)\n",
      "(2017, 11)\n",
      "(2017, 12)\n"
     ]
    }
   ],
   "source": [
    "tolerance_percent = 0.7\n",
    "\n",
    "for key in total_amt_per_mon_per_yr:\n",
    "    amount = total_amt_per_mon_per_yr[key]\n",
    "    if abs(amount - avg_amount_withdrawn) > avg_amount_withdrawn * tolerance_percent:\n",
    "        print(key)\n",
    "        # print(total_amt_per_mon_per_yr[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing outlier months from 2017 in test set </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10434\n",
      "1053\n"
     ]
    }
   ],
   "source": [
    "train_data = numeric_data[numeric_data['Year_2017'] == 0]\n",
    "test_data = numeric_data[numeric_data.apply(lambda x: x['Year_2017'] == 1\n",
    "                            and x['Month_10'] != 1 and x['Month_11'] != 1 and x['Month_12'] != 1, axis=1)]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "X_train = train_data.drop('Total amount Withdrawn', axis=1)\n",
    "y_train = train_data['Total amount Withdrawn']\n",
    "\n",
    "X_test = test_data.drop('Total amount Withdrawn', axis=1)\n",
    "y_test = test_data['Total amount Withdrawn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 351022.25503262907\n",
      "\tTraining Score = 0.38667187416152216\n",
      "\tTesting Score = -0.06821091445126548\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 351707.41076749505\n",
      "\tTraining Score = 0.4535210215154458\n",
      "\tTesting Score = -0.0723850379299813\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 351324.7551906196\n",
      "\tTraining Score = 0.453491696908386\n",
      "\tTesting Score = -0.07005281022397458\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 343412.7436405998\n",
      "\tTraining Score = 0.11927880563347205\n",
      "\tTesting Score = -0.022399249112364794\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 351678.3929187876\n",
      "\tTraining Score = 0.4535207159268131\n",
      "\tTesting Score = -0.0722080895796422\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 350563.3036768458\n",
      "\tTraining Score = 0.45328800801809777\n",
      "\tTesting Score = -0.06541943133494232\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 341482.3478579618\n",
      "\tTraining Score = -0.025157973486803265\n",
      "\tTesting Score = -0.010937309365951718\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 402572.9074124049\n",
      "\tTraining Score = 0.9434306386888788\n",
      "\tTesting Score = -0.40500152477546414\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 415344.7214716\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -0.49556432931859007\n"
     ]
    }
   ],
   "source": [
    "# testing_summary_all_atms_with_2017_non_outlier_month_data = compute_rmse_model_list(models, X_train, y_train, X_test, y_test)\n",
    "testing_summary['All ATMs Trained with 2017 Test Data (No Outlier Months)'] = compute_rmse_model_list(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> A slight improvement but pretty insignificant sadly </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Figuring out Outlier Days with 75% difference from the average amount withdrawn per day as the tolerance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_amount_withdrawn_per_day = sum(new_data['Total amount Withdrawn']) / len(new_data['Total amount Withdrawn'])\n",
    "tolerance_percent = 0.75\n",
    "\n",
    "outlier_day_data = new_data[new_data.apply(lambda x: abs(avg_amount_withdrawn_per_day - x['Total amount Withdrawn']) > \n",
    "                            avg_amount_withdrawn_per_day * tolerance_percent, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATM Name</th>\n",
       "      <th>Total amount Withdrawn</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Festival Religion</th>\n",
       "      <th>Working Day</th>\n",
       "      <th>Holiday Sequence</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Street ATM</td>\n",
       "      <td>123800</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KK Nagar ATM</td>\n",
       "      <td>945300</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Big Street ATM</td>\n",
       "      <td>52800</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>H</td>\n",
       "      <td>HHW</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Big Street ATM</td>\n",
       "      <td>88100</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWW</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KK Nagar ATM</td>\n",
       "      <td>1333100</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWW</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11558</th>\n",
       "      <td>Christ College ATM</td>\n",
       "      <td>1253100</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>H</td>\n",
       "      <td>WHH</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11567</th>\n",
       "      <td>KK Nagar ATM</td>\n",
       "      <td>1175200</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>HWW</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11577</th>\n",
       "      <td>KK Nagar ATM</td>\n",
       "      <td>997800</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWW</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11582</th>\n",
       "      <td>KK Nagar ATM</td>\n",
       "      <td>1154900</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWH</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11583</th>\n",
       "      <td>Christ College ATM</td>\n",
       "      <td>1120300</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>NH</td>\n",
       "      <td>W</td>\n",
       "      <td>WWH</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2204 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ATM Name  Total amount Withdrawn    Weekday  \\\n",
       "0          Big Street ATM                  123800   SATURDAY   \n",
       "3            KK Nagar ATM                  945300   SATURDAY   \n",
       "5          Big Street ATM                   52800     SUNDAY   \n",
       "10         Big Street ATM                   88100     MONDAY   \n",
       "13           KK Nagar ATM                 1333100     MONDAY   \n",
       "...                   ...                     ...        ...   \n",
       "11558  Christ College ATM                 1253100   SATURDAY   \n",
       "11567        KK Nagar ATM                 1175200     MONDAY   \n",
       "11577        KK Nagar ATM                  997800  WEDNESDAY   \n",
       "11582        KK Nagar ATM                 1154900   THURSDAY   \n",
       "11583  Christ College ATM                 1120300   THURSDAY   \n",
       "\n",
       "      Festival Religion Working Day Holiday Sequence  Month  Day  Year  \n",
       "0                     H           H              WHH      1    1  2011  \n",
       "3                     C           H              WHH      1    1  2011  \n",
       "5                    NH           H              HHW      2    1  2011  \n",
       "10                   NH           W              WWW      3    1  2011  \n",
       "13                   NH           W              WWW      3    1  2011  \n",
       "...                 ...         ...              ...    ...  ...   ...  \n",
       "11558                NH           H              WHH      9   23  2017  \n",
       "11567                NH           W              HWW      9   25  2017  \n",
       "11577                NH           W              WWW      9   27  2017  \n",
       "11582                NH           W              WWH      9   28  2017  \n",
       "11583                NH           W              WWH      9   28  2017  \n",
       "\n",
       "[2204 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_day_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinda useless, prints a really big list that's all\n",
    "# for idx, row in outlier_day_data.iterrows():\n",
    "#     print(\"{}, {}, {}\".format(row['Year'], row['Month'], row['Day']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I used value_counts() on Month, Day and Year of the outlier df but it contains data of multiple ATMs, so I split them to figure out the actual numbers </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ATM: Big Street ATM\n",
      "11    42\n",
      "3     42\n",
      "4     40\n",
      "1     40\n",
      "7     38\n",
      "8     34\n",
      "2     34\n",
      "6     31\n",
      "5     27\n",
      "12    25\n",
      "10    22\n",
      "9     20\n",
      "Name: Month, dtype: int64 \n",
      "\n",
      "For ATM: KK Nagar ATM\n",
      "1     95\n",
      "5     85\n",
      "8     84\n",
      "10    83\n",
      "7     83\n",
      "11    82\n",
      "3     82\n",
      "6     78\n",
      "4     77\n",
      "2     76\n",
      "9     74\n",
      "12    69\n",
      "Name: Month, dtype: int64 \n",
      "\n",
      "For ATM: Christ College ATM\n",
      "5     38\n",
      "1     35\n",
      "4     34\n",
      "3     33\n",
      "10    29\n",
      "8     26\n",
      "7     26\n",
      "6     26\n",
      "2     24\n",
      "11    16\n",
      "9     15\n",
      "12    10\n",
      "Name: Month, dtype: int64 \n",
      "\n",
      "For ATM: Airport ATM\n",
      "1     23\n",
      "7     22\n",
      "5     19\n",
      "3     18\n",
      "6     17\n",
      "2     17\n",
      "11    16\n",
      "10    16\n",
      "4     15\n",
      "9     14\n",
      "8     14\n",
      "12    11\n",
      "Name: Month, dtype: int64 \n",
      "\n",
      "For ATM: Mount Road ATM\n",
      "6     40\n",
      "4     39\n",
      "5     36\n",
      "3     36\n",
      "1     34\n",
      "8     26\n",
      "7     25\n",
      "11    24\n",
      "10    22\n",
      "2     20\n",
      "9     14\n",
      "12    11\n",
      "Name: Month, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for atm in outlier_day_data['ATM Name'].unique():\n",
    "    print(\"For ATM:\", atm)\n",
    "    print(outlier_day_data[outlier_day_data['ATM Name'] == atm]['Month'].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Basically KK Nagar ATM has vastly large number of outlier days as compared to the others and Airport ATM is the best in this regard </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ATM: Big Street ATM\n",
      "2011    243\n",
      "2012     78\n",
      "2016     30\n",
      "2017     24\n",
      "2014      8\n",
      "2015      7\n",
      "2013      5\n",
      "Name: Year, dtype: int64 \n",
      "\n",
      "For ATM: KK Nagar ATM\n",
      "2013    260\n",
      "2014    258\n",
      "2012    157\n",
      "2016     86\n",
      "2017     85\n",
      "2015     69\n",
      "2011     53\n",
      "Name: Year, dtype: int64 \n",
      "\n",
      "For ATM: Christ College ATM\n",
      "2017    92\n",
      "2014    75\n",
      "2016    69\n",
      "2015    34\n",
      "2013    19\n",
      "2012    12\n",
      "2011    11\n",
      "Name: Year, dtype: int64 \n",
      "\n",
      "For ATM: Airport ATM\n",
      "2016    68\n",
      "2017    54\n",
      "2015    35\n",
      "2014    17\n",
      "2013    14\n",
      "2012    11\n",
      "2011     3\n",
      "Name: Year, dtype: int64 \n",
      "\n",
      "For ATM: Mount Road ATM\n",
      "2016    106\n",
      "2015     54\n",
      "2014     54\n",
      "2017     50\n",
      "2012     30\n",
      "2013     24\n",
      "2011      9\n",
      "Name: Year, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for atm in outlier_day_data['ATM Name'].unique():\n",
    "    print(\"For ATM:\", atm)\n",
    "    print(outlier_day_data[outlier_day_data['ATM Name'] == atm]['Year'].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training all models for each ATM separately and the results might shock you! </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_per_atm(atm_name):\n",
    "    models = {'K-Nearest Neighbours': KNeighborsRegressor(n_neighbors=37, n_jobs=-1), 'Vanilla Linear Regression': LinearRegression(), \n",
    "          'Ridge Linear Regression': Ridge(), 'Elasticnet Linear Regression': ElasticNet(), \n",
    "          'Lasso Linear Regression': Lasso(max_iter=1100), 'Bayesian Ridge Linear Regression': BayesianRidge(), \n",
    "          'Support Vector Regression': SVR(), 'Random Forest Regression': RandomForestRegressor(), \n",
    "          'Decision Tree Regression': DecisionTreeRegressor()}\n",
    "    \n",
    "    categorical_features_list = ['Weekday', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'Month', 'Day', 'Year']\n",
    "    \n",
    "    curr_atm_data = new_data[new_data['ATM Name'] == atm_name].drop('ATM Name', axis=1)\n",
    "    numeric_curr_atm_data = convert_categorical_to_numerical(curr_atm_data, categorical_features_list)\n",
    "    \n",
    "    train_data = numeric_curr_atm_data[numeric_curr_atm_data['Year_2017'] == 0]\n",
    "    test_data = numeric_curr_atm_data[numeric_curr_atm_data['Year_2017'] == 1]\n",
    "    \n",
    "    ''' Finding Outlier Months for each bank separately\n",
    "    print(\"\\nOutlier Months for bank {} are as follows,\".format(bank_name))\n",
    "    total_amt_per_mon_per_yr = {}\n",
    "    for year in curr_bank_data['Year'].unique():\n",
    "       for month in curr_bank_data['Month'].unique():\n",
    "           total_amt_per_mon_per_yr[(year, month)] = sum(curr_bank_data[curr_bank_data.apply(lambda x: x['Year'] == year\n",
    "                               and x['Month'] == month, axis=1)]['Total amount Withdrawn'])\n",
    "\n",
    "    total_amount_withdrawn = sum(total_amt_per_mon_per_yr.values())\n",
    "    avg_amount_withdrawn = sum(total_amt_per_mon_per_yr.values()) / len(total_amt_per_mon_per_yr.values())\n",
    "    # print(\"Total Amount Withdrawn:\", total_amount_withdrawn)\n",
    "    # print(\"Average Amount Withdrawn:\", avg_amount_withdrawn)\n",
    "\n",
    "    tolerance_percent = 0.7\n",
    "\n",
    "    for key in total_amt_per_mon_per_yr:\n",
    "        amount = total_amt_per_mon_per_yr[key]\n",
    "        if abs(amount - avg_amount_withdrawn) > avg_amount_withdrawn * tolerance_percent:\n",
    "            print(key)\n",
    "    '''\n",
    "    \n",
    "    # This takes 2016 data as test data and ignores 2017 data completely\n",
    "    # train_data = numeric_curr_bank_data[numeric_curr_bank_data.apply(lambda x: x['Year_2017'] == 0 and \n",
    "    #                                                                 x['Year_2016'] == 0, axis=1)]\n",
    "    # test_data = numeric_curr_bank_data[numeric_curr_bank_data['Year_2016'] == 1]\n",
    "    # Conclusions: For some reason, Airport ATM accuracy increases slightly somehow but everything else gets rekt\n",
    "    \n",
    "    \n",
    "    print(\"\\nFor ATM:\", atm_name)\n",
    "    print(\"Number of training rows:\",len(train_data))\n",
    "    print(\"Number of testing rows:\", len(test_data))\n",
    "    print()\n",
    "\n",
    "    X_train = train_data.drop('Total amount Withdrawn', axis=1)\n",
    "    y_train = train_data['Total amount Withdrawn']\n",
    "\n",
    "    X_test = test_data.drop('Total amount Withdrawn', axis=1)\n",
    "    y_test = test_data['Total amount Withdrawn']\n",
    "    \n",
    "    testing_dict_curr_atm = compute_rmse_model_list(models, X_train, y_train, X_test, y_test)\n",
    "    return testing_dict_curr_atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For ATM: Big Street ATM\n",
      "Number of training rows: 2117\n",
      "Number of testing rows: 237\n",
      "\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 181461.59767223545\n",
      "\tTraining Score = 0.28999562819250424\n",
      "\tTesting Score = -0.36556725432459025\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 276512.2844681913\n",
      "\tTraining Score = 0.5444464813526175\n",
      "\tTesting Score = -2.1708266083172028\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 273587.69620255945\n",
      "\tTraining Score = 0.5442115519020725\n",
      "\tTesting Score = -2.104107536431348\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 160736.02690753856\n",
      "\tTraining Score = 0.13017936078590775\n",
      "\tTesting Score = -0.07144551642773656\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 276469.1320024553\n",
      "\tTraining Score = 0.5444461398934355\n",
      "\tTesting Score = -2.169837008089656\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 270701.1207086409\n",
      "\tTraining Score = 0.5436357614309526\n",
      "\tTesting Score = -2.038951303826138\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 158495.8194715498\n",
      "\tTraining Score = -0.0014462945200466315\n",
      "\tTesting Score = -0.04178777477528839\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 274161.7560782836\n",
      "\tTraining Score = 0.9299910473100148\n",
      "\tTesting Score = -2.117147692166071\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 274156.7397174278\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -2.117033623779327\n",
      "\n",
      "For ATM: Mount Road ATM\n",
      "Number of training rows: 2021\n",
      "Number of testing rows: 235\n",
      "\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 343922.66193841596\n",
      "\tTraining Score = 0.3529602657639054\n",
      "\tTesting Score = -2.4281120216633076\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 322167.22468273697\n",
      "\tTraining Score = 0.5181025322944846\n",
      "\tTesting Score = -2.0081267367669327\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 323206.278030127\n",
      "\tTraining Score = 0.5169168741625048\n",
      "\tTesting Score = -2.0275616405234143\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 325292.86913239764\n",
      "\tTraining Score = 0.15649508138830026\n",
      "\tTesting Score = -2.0667791651930627\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 322203.5496961272\n",
      "\tTraining Score = 0.5181017969667947\n",
      "\tTesting Score = -2.0088051198912362\n",
      "For"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318209236896.22656, tolerance: 12393046096.022762\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 324014.4789707869\n",
      "\tTraining Score = 0.5135989086530637\n",
      "\tTesting Score = -2.0427218501556643\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 322190.0963087881\n",
      "\tTraining Score = -0.00016256288294114185\n",
      "\tTesting Score = -2.0085538639651888\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 329908.39907994063\n",
      "\tTraining Score = 0.9217026970133028\n",
      "\tTesting Score = -2.1544246849018993\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 351033.3047786631\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -2.571330670147686\n",
      "\n",
      "For ATM: Airport ATM\n",
      "Number of training rows: 2058\n",
      "Number of testing rows: 195\n",
      "\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 280998.75391993637\n",
      "\tTraining Score = 0.21011877894017894\n",
      "\tTesting Score = -0.320081646974691\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 270419.38396096433\n",
      "\tTraining Score = 0.32222981007075846\n",
      "\tTesting Score = -0.2225528522966016\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 270340.7285163081\n",
      "\tTraining Score = 0.3217569895030462\n",
      "\tTesting Score = -0.22184176086544904\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 274810.90552300756\n",
      "\tTraining Score = 0.09953188228525989\n",
      "\tTesting Score = -0.26258298215092557\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 270427.41003619053\n",
      "\tTraining Score = 0.3222292592137085\n",
      "\tTesting Score = -0.22262542436323265\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 270597.6230639246\n",
      "\tTraining Score = 0.3157519731139333\n",
      "\tTesting Score = -0.2241650040142349\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 277099.6585735705\n",
      "\tTraining Score = -0.00025917436172195885\n",
      "\tTesting Score = -0.2837013158547954\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 276852.8230632613\n",
      "\tTraining Score = 0.8899228192952251\n",
      "\tTesting Score = -0.28141533727954293\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 284015.8306349878\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -0.3485811978764737\n",
      "\n",
      "For ATM: KK Nagar ATM\n",
      "Number of training rows: 2123\n",
      "Number of testing rows: 248\n",
      "\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 509357.0695052921\n",
      "\tTraining Score = 0.4036695621588333\n",
      "\tTesting Score = -0.8766772926341135\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 457547.2389128469\n",
      "\tTraining Score = 0.5157346139328643\n",
      "\tTesting Score = -0.5143170094807608\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 452583.4106613807\n",
      "\tTraining Score = 0.5146716060699499\n",
      "\tTesting Score = -0.4816382614965147\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 505898.64455021545\n",
      "\tTraining Score = 0.18357797183729763\n",
      "\tTesting Score = -0.8512793392665452\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 457459.42343176226\n",
      "\tTraining Score = 0.5157343902470022\n",
      "\tTesting Score = -0.5137357898638595\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 449447.70286618423\n",
      "\tTraining Score = 0.5109456907699065\n",
      "\tTesting Score = -0.46117843124706037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4886918595366.969, tolerance: 40041415972.54263\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Support Vector Regression\n",
      "\tTesting RMSE = 476342.10707451584\n",
      "\tTraining Score = -0.010162086368770362\n",
      "\tTesting Score = -0.641280726160238\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 448783.7496259357\n",
      "\tTraining Score = 0.9302856073280882\n",
      "\tTesting Score = -0.4568645252661532\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 475371.07610791805\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -0.6345959928147875\n",
      "\n",
      "For ATM: Christ College ATM\n",
      "Number of training rows: 2115\n",
      "Number of testing rows: 240\n",
      "\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 454068.21878890437\n",
      "\tTraining Score = 0.2887955781878394\n",
      "\tTesting Score = -0.48406666444966473\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 563369.5987146889\n",
      "\tTraining Score = 0.4884805976915243\n",
      "\tTesting Score = -1.2845360365183236\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 559635.1538683609\n",
      "\tTraining Score = 0.487886260442293\n",
      "\tTesting Score = -1.254349113835958\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 437212.60466702207\n",
      "\tTraining Score = 0.15157031881129213\n",
      "\tTesting Score = -0.37593065437858364\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 563333.8566610938\n",
      "\tTraining Score = 0.4884802021606778\n",
      "\tTesting Score = -1.2842461684834077\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 554222.9690183143\n",
      "\tTraining Score = 0.48620201491936166\n",
      "\tTesting Score = -1.2109567113667454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 303677188912.1094, tolerance: 13354579722.531445\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Support Vector Regression\n",
      "\tTesting RMSE = 464427.1776615809\n",
      "\tTraining Score = -0.01517727140145797\n",
      "\tTesting Score = -0.552553057061139\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 571168.3598554188\n",
      "\tTraining Score = 0.9174690716103661\n",
      "\tTesting Score = -1.3482237780287183\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 576741.9604988005\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -1.394276468402862\n"
     ]
    }
   ],
   "source": [
    "for atm_name in new_data['ATM Name'].unique():\n",
    "    testing_dict_curr_atm = model_training_per_atm(atm_name)\n",
    "    testing_summary[atm_name + ' trained with 2017 Data'] = testing_dict_curr_atm\n",
    "    \n",
    "# Note: Bayesian Ridge Linear Regression does not converge for the Mount Road and KK Nagar models and increasing n_iter\n",
    "# to even 5000 (default is 350) didn't change that at all, so that's sad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I know the output is a bit long, but read it for cool insights and damn Big Street ATM is the OP right now, SVR got 1 lakh 58 thousand RMSE on it </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Upto this point, removing outlier months from test set has had no effect, be it for the model with all ATMs or for individual ATMs, infact it made the RMSE worse at times, so I think we can stop pursuing the outlier month part for now </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hyperparameter Tuning </h2>\n",
    "<p> Right now, just focused on SVR as it performs the best consistently </p>\n",
    "<p> Note that the function below this cell takes the bank_name and the random search or grid search object and does all the tasks of converting to numeric data, train test split using 2017 data and then does Cross Validation and then computes the RMSE on the best estimator found after hyperparameter search </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works for both RandomizedSearchCV() object and GridSearchCV() object\n",
    "def model_training_hyperparam_per_atm(atm_name, param_cv_obj):\n",
    "    categorical_features_list = ['Weekday', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'Month', 'Day', 'Year']\n",
    "    \n",
    "    curr_atm_data = new_data[new_data['ATM Name'] == atm_name].drop('ATM Name', axis=1)\n",
    "    numeric_curr_atm_data = convert_categorical_to_numerical(curr_atm_data, categorical_features_list)\n",
    "    \n",
    "    train_data = numeric_curr_atm_data[numeric_curr_atm_data['Year_2017'] == 0]\n",
    "    test_data = numeric_curr_atm_data[numeric_curr_atm_data['Year_2017'] == 1]\n",
    "    \n",
    "    # test_indices_to_keep = sorted(list(set(temp_test_data.index) - set(outlier_day_data.index)))\n",
    "    \n",
    "    # print(test_indices_to_keep)\n",
    "    # print(temp_test_data.index)\n",
    "    # test_data = temp_test_data.take(test_indices_to_keep)\n",
    "    \n",
    "    X_train = train_data.drop('Total amount Withdrawn', axis=1)\n",
    "    y_train = train_data['Total amount Withdrawn']\n",
    "\n",
    "    X_test = test_data.drop('Total amount Withdrawn', axis=1)\n",
    "    y_test = test_data['Total amount Withdrawn']\n",
    "    \n",
    "    param_cv_obj.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\\n\", param_cv_obj.best_params_)\n",
    "    print(\"\\nBest Score:\", param_cv_obj.best_score_)\n",
    "    \n",
    "    best_model = param_cv_obj.best_estimator_\n",
    "    model_predictions = best_model.predict(X_test)\n",
    "    model_rmse = np.sqrt(mean_squared_error(y_test, model_predictions))\n",
    "    \n",
    "    print(\"Test RMSE:\", model_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svr_param_grid = {'C': list(np.logspace(np.log10(0.001), np.log10(1000), num=25)),\n",
    "                         'epsilon': list(np.logspace(np.log10(0.001), np.log10(1000), num=25))}\n",
    "\n",
    "random_cv_svr = RandomizedSearchCV(SVR(), svr_param_grid, n_iter=50, cv=10, verbose=5, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "grid_cv_svr = GridSearchCV(SVR(), svr_param_grid, verbose=5, n_jobs=-1, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SVR Randomized Search for all ATMs individually </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'epsilon': 177.82794100389228, 'C': 1000.0}\n",
      "\n",
      "Best Score: -248425.02133705173\n",
      "Test RMSE: 458465.7883776411\n"
     ]
    }
   ],
   "source": [
    "model_training_hyperparam_per_atm('Christ College ATM', random_cv_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> There's a big difference in the CV and Test RMSE for Christ College, so it probably means that our test set is not so good considering we are not using normal train-test split </h3>\n",
    "\n",
    "<p> CV RMSE: -248436.58633545385 <br> Test RMSE: 458393.93110469 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Random Search with SVR on Big Street ATM \n",
      "\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'epsilon': 0.01, 'C': 1000.0}\n",
      "\n",
      "Best Score: -157505.96471867283\n",
      "Test RMSE: 158667.2245605045\n",
      "\n",
      "Performing Random Search with SVR on Mount Road ATM \n",
      "\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   26.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   47.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'epsilon': 0.0017782794100389228, 'C': 1000.0}\n",
      "\n",
      "Best Score: -240273.563879369\n",
      "Test RMSE: 321926.99321987526\n",
      "\n",
      "Performing Random Search with SVR on Airport ATM \n",
      "\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   48.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'epsilon': 5.623413251903491, 'C': 1000.0}\n",
      "\n",
      "Best Score: -193097.65343665896\n",
      "Test RMSE: 275428.75423243525\n",
      "\n",
      "Performing Random Search with SVR on KK Nagar ATM \n",
      "\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'epsilon': 56.23413251903491, 'C': 562.341325190349}\n",
      "\n",
      "Best Score: -437791.51244166447\n",
      "Test RMSE: 477182.97760147636\n",
      "\n",
      "Performing Random Search with SVR on Christ College ATM \n",
      "\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'epsilon': 0.01, 'C': 1000.0}\n",
      "\n",
      "Best Score: -248436.60000969804\n",
      "Test RMSE: 458393.7021660285\n"
     ]
    }
   ],
   "source": [
    "for atm_name in atm_names:\n",
    "    print(\"\\nPerforming Random Search with SVR on\", atm_name, \"\\n\")\n",
    "    # model_training_hyperparam_per_atm(atm_name, grid_cv_svr)\n",
    "    model_training_hyperparam_per_atm(atm_name, random_cv_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training Model on All ATMs but using Test Data from individual ATMs </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_atm_model_training_with_different_test_atms(data):\n",
    "    models = {'K-Nearest Neighbours': KNeighborsRegressor(n_neighbors=37, n_jobs=-1), 'Vanilla Linear Regression': LinearRegression(), \n",
    "          'Ridge Linear Regression': Ridge(), 'Elasticnet Linear Regression': ElasticNet(), \n",
    "          'Lasso Linear Regression': Lasso(max_iter=1100), 'Bayesian Ridge Linear Regression': BayesianRidge(), \n",
    "          'Support Vector Regression': SVR(), 'Random Forest Regression': RandomForestRegressor(), \n",
    "          'Decision Tree Regression': DecisionTreeRegressor()}\n",
    "    \n",
    "    categorical_features_list = ['ATM Name', 'Weekday', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'Month', 'Day', 'Year']\n",
    "    atm_names = data['ATM Name'].unique()\n",
    "    \n",
    "    numeric_data = convert_categorical_to_numerical(data, categorical_features_list)\n",
    "    \n",
    "    train_data = numeric_data[numeric_data['Year_2017'] == 0]\n",
    "    test_data = numeric_data[numeric_data['Year_2017'] == 1]\n",
    "    \n",
    "    for atm_name in atm_names:\n",
    "        print(\"\\nFor ATM:\", atm_name)\n",
    "        \n",
    "        # I know this is sad jugaad, but its honest work and yeah Airport ATM column gets dropped in pd.get_dummies\n",
    "        # so I'm just using that knowledge to hardcode column names here\n",
    "        # if atm_name != 'Airport ATM':\n",
    "        '''\n",
    "            if 'ATM Name_' + atm_name in test_data.columns:\n",
    "                curr_atm_test_data = test_data[test_data['ATM Name_' + atm_name] == 1]\n",
    "            else:\n",
    "                curr_atm_test_data = test_data[test_data.apply(\n",
    "                    lambda x: x['ATM Name_Big Street ATM'] == 0 and x['ATM Name_Mount Road ATM'] == 0\n",
    "                            and x['ATM Name_KK Nagar ATM'] == 0 and x['ATM Name_Christ College ATM'] == 0, axis=1)]\n",
    "\n",
    "                curr_atm_test_data = test_data[test_data.apply(\n",
    "                    lambda x: [x[a_name] == 0 for a_name in atm_names], axis=1)]\n",
    "        '''\n",
    "        \n",
    "        X_train = train_data.drop(['Total amount Withdrawn', 'ATM Name_' + atm_names[0]], axis=1)\n",
    "        y_train = train_data['Total amount Withdrawn']\n",
    "        \n",
    "        curr_atm_test_data = test_data[test_data['ATM Name_' + atm_name] == 1]\n",
    "        X_test = curr_atm_test_data.drop(['Total amount Withdrawn', 'ATM Name_' + atm_names[0]], axis=1)\n",
    "        y_test = curr_atm_test_data['Total amount Withdrawn']\n",
    "\n",
    "        compute_rmse_model_list(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Tbh idk what to make of this result, its weird. In some ways, the trash ATMs are performing trash but some models are performing quite well on Mount Road ATM which didn't happen with the individual model, so I guess KK and Christ College are the ones that we need to destroy </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For ATM: Big Street ATM\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 160761.22195743438\n",
      "\tTraining Score = 0.4327735808352957\n",
      "\tTesting Score = -0.07178143662536951\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 220189.09903158937\n",
      "\tTraining Score = 0.4535210215154458\n",
      "\tTesting Score = -1.010644370788854\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 219278.1361879842\n",
      "\tTraining Score = 0.4534921286886008\n",
      "\tTesting Score = -0.9940419736478675\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 230626.41352311775\n",
      "\tTraining Score = 0.146904990423854\n",
      "\tTesting Score = -1.2057776103883073\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 220135.20854777785\n",
      "\tTraining Score = 0.4535207159235627\n",
      "\tTesting Score = -1.009660295385542\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 216769.0031492237\n",
      "\tTraining Score = 0.4532224416363815\n",
      "\tTesting Score = -0.9486686321193216\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 206796.59945379852\n",
      "\tTraining Score = -0.02521686557734082\n",
      "\tTesting Score = -0.7734968418030763\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 274304.36817749985\n",
      "\tTraining Score = 0.9439913214639732\n",
      "\tTesting Score = -2.1203914603880505\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 275555.47819120093\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -2.148920761973852\n",
      "\n",
      "For ATM: Mount Road ATM\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 296066.96709983225\n",
      "\tTraining Score = 0.4327735808352957\n",
      "\tTesting Score = -1.5404656530651817\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 236570.5400194382\n",
      "\tTraining Score = 0.4535210215154458\n",
      "\tTesting Score = -0.6220145307364398\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 236915.40112081115\n",
      "\tTraining Score = 0.4534921286886008\n",
      "\tTesting Score = -0.626746966385769\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 305282.52403376804\n",
      "\tTraining Score = 0.146904990423854\n",
      "\tTesting Score = -1.7010791183983565\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 236594.61542081521\n",
      "\tTraining Score = 0.4535207159235627\n",
      "\tTesting Score = -0.6223446871280552\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 237782.9024078032\n",
      "\tTraining Score = 0.4532224416363815\n",
      "\tTesting Score = -0.638681933465496\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 275806.51085791795\n",
      "\tTraining Score = -0.02521686557734082\n",
      "\tTesting Score = -1.2046641818844228\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 330703.7079768831\n",
      "\tTraining Score = 0.9438507467957216\n",
      "\tTesting Score = -2.169651735385989\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 351862.43485747353\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -2.588221344914708\n",
      "\n",
      "For ATM: Airport ATM\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 266209.1384627837\n",
      "\tTraining Score = 0.4327735808352957\n",
      "\tTesting Score = -0.1847805742958737\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 252359.49924401758\n",
      "\tTraining Score = 0.4535210215154458\n",
      "\tTesting Score = -0.06470996540650753\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 252100.3859773988\n",
      "\tTraining Score = 0.4534921286886008\n",
      "\tTesting Score = -0.06252467936732997\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 296236.674509219\n",
      "\tTraining Score = 0.146904990423854\n",
      "\tTesting Score = -0.46713356366525716\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 252329.5499923005\n",
      "\tTraining Score = 0.4535207159235627\n",
      "\tTesting Score = -0.0644572673728403\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 251569.18901773062\n",
      "\tTraining Score = 0.4532224416363815\n",
      "\tTesting Score = -0.058051736993591334\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 287718.95480246906\n",
      "\tTraining Score = -0.02521686557734082\n",
      "\tTesting Score = -0.383977255687874\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 278294.46870485577\n",
      "\tTraining Score = 0.943420400836649\n",
      "\tTesting Score = -0.29479541862804526\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 283418.7089168649\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -0.3429165797203906\n",
      "\n",
      "For ATM: KK Nagar ATM\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 439900.6725003991\n",
      "\tTraining Score = 0.4327735808352957\n",
      "\tTesting Score = -0.39976192001857025\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 442596.94348232786\n",
      "\tTraining Score = 0.4535210215154458\n",
      "\tTesting Score = -0.4169735500121525\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 442082.9970781393\n",
      "\tTraining Score = 0.4534921286886008\n",
      "\tTesting Score = -0.4136846631448141\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 390643.4303126342\n",
      "\tTraining Score = 0.146904990423854\n",
      "\tTesting Score = -0.10383960587255747\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 442540.1409016468\n",
      "\tTraining Score = 0.4535207159235627\n",
      "\tTesting Score = -0.4166098665718949\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 441077.7848924154\n",
      "\tTraining Score = 0.4532224416363815\n",
      "\tTesting Score = -0.40726307508050114\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 371826.3852616734\n",
      "\tTraining Score = -0.02521686557734082\n",
      "\tTesting Score = -5.833060162729886e-05\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 446327.13320062077\n",
      "\tTraining Score = 0.9424640307539619\n",
      "\tTesting Score = -0.4409585936366476\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 475790.6825790436\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -0.6374829575647274\n",
      "\n",
      "For ATM: Christ College ATM\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 486451.6394307868\n",
      "\tTraining Score = 0.4327735808352957\n",
      "\tTesting Score = -0.7032976262850792\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 486682.6738847267\n",
      "\tTraining Score = 0.4535210215154458\n",
      "\tTesting Score = -0.7049159326243717\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 486305.995228053\n",
      "\tTraining Score = 0.4534921286886008\n",
      "\tTesting Score = -0.7022778402769598\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 447038.859598982\n",
      "\tTraining Score = 0.146904990423854\n",
      "\tTesting Score = -0.43847312213602674\n",
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 486655.7240515841\n",
      "\tTraining Score = 0.4535207159235627\n",
      "\tTesting Score = -0.7047271199535323\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 485343.5132790658\n",
      "\tTraining Score = 0.4532224416363815\n",
      "\tTesting Score = -0.6955463158362263\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 477410.79981771\n",
      "\tTraining Score = -0.02521686557734082\n",
      "\tTesting Score = -0.6405734423324327\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 570506.033794626\n",
      "\tTraining Score = 0.9431464345609663\n",
      "\tTesting Score = -1.342780941352073\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 573085.5795239777\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -1.3640146273849705\n"
     ]
    }
   ],
   "source": [
    "all_atm_model_training_with_different_test_atms(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Doing the same as above but just using data from Big Street, Mount Road and Airport ATMs </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For ATM: Big Street ATM\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 158618.10337009552\n",
      "\tTraining Score = 0.3840104394960905\n",
      "\tTesting Score = -0.043395930938965765\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 188278.79850039512\n",
      "\tTraining Score = 0.3888530973235256\n",
      "\tTesting Score = -0.4700986198565593\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 187658.3412392558\n",
      "\tTraining Score = 0.38878449463634546\n",
      "\tTesting Score = -0.4604254071488416\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 167807.48386731514\n",
      "\tTraining Score = 0.13052242796752922\n",
      "\tTesting Score = -0.16779412109203484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42250940223.5625, tolerance: 31405633558.079407\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 188247.22492758566\n",
      "\tTraining Score = 0.3888525917818654\n",
      "\tTesting Score = -0.46960560232858234\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 185031.07441716632\n",
      "\tTraining Score = 0.3878119473737709\n",
      "\tTesting Score = -0.4198189711009328\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 174880.2829468054\n",
      "\tTraining Score = -0.0013433259042736623\n",
      "\tTesting Score = -0.2683097411069155\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 274300.34988959227\n",
      "\tTraining Score = 0.9301733587889188\n",
      "\tTesting Score = -2.120300039755443\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 274079.3766881018\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -2.115274709222968\n",
      "\n",
      "For ATM: Mount Road ATM\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 294469.2194498337\n",
      "\tTraining Score = 0.3840104394960905\n",
      "\tTesting Score = -1.513120011264927\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 262493.3873361208\n",
      "\tTraining Score = 0.3888530973235256\n",
      "\tTesting Score = -0.9969636172379583\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 262913.9947634632\n",
      "\tTraining Score = 0.38878449463634546\n",
      "\tTesting Score = -1.0033684312945792\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 259961.59685498953\n",
      "\tTraining Score = 0.13052242796752922\n",
      "\tTesting Score = -0.9586273293803254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42250940223.5625, tolerance: 31405633558.079407\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 262515.15381622716\n",
      "\tTraining Score = 0.3888525917818654\n",
      "\tTesting Score = -0.9972948154557034\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 264676.80225273105\n",
      "\tTraining Score = 0.3878119473737709\n",
      "\tTesting Score = -1.0303231942122686\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 237358.8875030304\n",
      "\tTraining Score = -0.0013433259042736623\n",
      "\tTesting Score = -0.6328429429423257\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 330958.62926677166\n",
      "\tTraining Score = 0.9293786937421639\n",
      "\tTesting Score = -2.1745402389936923\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 353042.5494216351\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -2.6123308430175896\n",
      "\n",
      "For ATM: Airport ATM\n",
      "RMSEs for each model\n",
      "For K-Nearest Neighbours\n",
      "\tTesting RMSE = 265539.25392855983\n",
      "\tTraining Score = 0.3840104394960905\n",
      "\tTesting Score = -0.1788253497447605\n",
      "For Vanilla Linear Regression\n",
      "\tTesting RMSE = 247679.74538280914\n",
      "\tTraining Score = 0.3888530973235256\n",
      "\tTesting Score = -0.02558814034151591\n",
      "For Ridge Linear Regression\n",
      "\tTesting RMSE = 247664.56194176135\n",
      "\tTraining Score = 0.38878449463634546\n",
      "\tTesting Score = -0.025462401519049882\n",
      "For Elasticnet Linear Regression\n",
      "\tTesting RMSE = 262243.28053838445\n",
      "\tTraining Score = 0.13052242796752922\n",
      "\tTesting Score = -0.14974291809005313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42250940223.5625, tolerance: 31405633558.079407\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Lasso Linear Regression\n",
      "\tTesting RMSE = 247683.33469720022\n",
      "\tTraining Score = 0.3888525917818654\n",
      "\tTesting Score = -0.025617865702699305\n",
      "For Bayesian Ridge Linear Regression\n",
      "\tTesting RMSE = 247814.57944030146\n",
      "\tTraining Score = 0.3878119473737709\n",
      "\tTesting Score = -0.026705081496683825\n",
      "For Support Vector Regression\n",
      "\tTesting RMSE = 262533.8514480243\n",
      "\tTraining Score = -0.0013433259042736623\n",
      "\tTesting Score = -0.15229220690026346\n",
      "For Random Forest Regression\n",
      "\tTesting RMSE = 276778.72441148956\n",
      "\tTraining Score = 0.9299957724280646\n",
      "\tTesting Score = -0.28072949675867287\n",
      "For Decision Tree Regression\n",
      "\tTesting RMSE = 286474.5530382437\n",
      "\tTraining Score = 1.0\n",
      "\tTesting Score = -0.37203157502015816\n"
     ]
    }
   ],
   "source": [
    "good_data = new_data[new_data.apply(lambda x: x['ATM Name'] != 'KK Nagar ATM' \n",
    "                                    and x['ATM Name'] != 'Christ College ATM', axis=1)]\n",
    "\n",
    "all_atm_model_training_with_different_test_atms(good_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Conclusions: </h3>\n",
    "<p> For Big Street ATM: Before, Best Performance = 160761 by KNN with k = 37. Now, it is 158618 also by KNN with k = 37, so an improvement</p>\n",
    "<p> For Mount Road ATM: Before, Best Performance = 236570 by Vanilla LR with k = 37. Now, it is 237358 by SVR, so performance worsened somehow </p>\n",
    "<p> For Airport ATM: Before, Best Performance = 251569 by Bayesian Ridge. Now, it is 247664 by Ridge, so performance increased </p>\n",
    "<p> In summary, we got an improvement of 2k for Big Street, a decrement of 1k for Mount Road and an increment of 4k for Airport. So its definitely helping improve our cause but man that's still very marginal improvement </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hyperparameter Tuning for Lasso </h3>\n",
    "<p> Alpha is the only tunable parameter here </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Plotting Validation Curve For Lasso Regression => Train Error vs Cross Validation Error (Y-Axis) wrt alpha (X-Axis) </h3>\n",
    "\n",
    "<p> Note that the validation_curve() method returns the train_score and validation_scores. These scores are an array of dimension (n_ticks, n_cv_folds). I chose cv=10, so n_cv_folds=10, n_ticks = number of values that the parameter being tested again takes, here that parameter is alpha and I used 50 values of alpha in the range 10^-3 to 10^3 (log-spaced) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 500 | elapsed:   28.4s remaining:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   48.8s finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'pop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-1975d319d983>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Lasso Validation Curve For\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matm_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"alpha\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mtitle\u001b[1;34m(label, fontdict, loc, pad, **kwargs)\u001b[0m\n\u001b[0;32m   3053\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3054\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3055\u001b[1;33m     return gca().set_title(\n\u001b[0m\u001b[0;32m   3056\u001b[0m         label, fontdict=fontdict, loc=loc, pad=pad, **kwargs)\n\u001b[0;32m   3057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mset_title\u001b[1;34m(self, label, fontdict, loc, pad, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfontdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasvin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\matplotlib\\text.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# Update bbox last, as it depends on font properties.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0msentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# bbox can be None, so use another sentinel.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bbox\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentinel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbbox\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msentinel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'pop'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAE/CAYAAACw445JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAXJklEQVR4nO3df7DldX3f8ddbkAQRRQGt/FKaoEgyYPWKxqmVxJiwakKc0QioVJJIqBrt1DbSztRYmSTNTGIcowaJpWhbJU6lBhUlaVq1LWJYLKKo6BZUNmBYRBTRiAvv/nG+mMPN/XDPLnfvXZfHY+YO93y/3/M973P3O7tPvvd7zqnuDgAA8Pc9YKMHAACA3ZVYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAa6CqHlNVXVV7T7c/XFX/dJFtd+Kx/k1VveO+zAvAYsQysO6q6stV9bMbPce8qrqkqt6wwvKTquprOxq23b2pu9+5BnOdUFVbl+37d7r71+7rvgeP96iq+g9VdWNV3VZVX6iqf1dV++2Kx9vB2bqqbq+qb09ft270TMCeTywDzJyf5CVVVcuWvyTJf+nu7es/0vqqqocn+USSfZP8VHfvn+RZSQ5I8mM7sb+dOnO+iuO6+8HT1wG7yUzAHkwsA7uNqnpYVX2wqrZV1Tem7w+bW//Sqrp2OuN5XVW9aFr+41X1sar6ZlXdXFV/Onefp1XV5dO6y6vqaYOHf3+Shyd5+vw8SZ6b5F3T7edU1f+tqm9V1fVV9fp7eS4frapfm77fq6p+f5rt2iTPWbbt6VX1+el5XVtVvz4t3y/Jh5McMnc29ZCqen1V/ee5+/9iVV1dVbdOj/v4uXVfrqp/WVVXTT+DP62qHx2M/S+S3Jbkxd395STp7uu7+9XdfdVKl48se54vrar/U1V/WFW3JDl7mukn57Y/uKq+W1WPmG4/t6qunLa7tKqOHf1M701VvayqtlTVLVV1UVUdMreuq+oVVfWlJF/amf0D919iGdidPCDJf0zy6CRHJPlukrckPwjHNyfZNJ3xfFqSK6f7nZ3kz5M8LMlhSf5ous/Dk3xout+BSd6Y5ENVdeDyB+7u7yZ5b5LT5hb/cpIvdPenp9u3T+sPyCx4/1lV/dICz+tlmUX3P0qylOT5y9bfNK1/SJLTk/xhVT2xu29PsinJDXNnU2+Yv2NVPTbJe5L88yQHJ7k4yQeqap9lz+PEJEcmOTbJSwdz/mySC7v7rgWe08hTklyb5BFJ3pDkwiSnLJvlY919U1U9Mcl5SX49sz+ftye5qKp+ZEcesKp+JsnvTvt+VJKvJLlg2Wa/NM12zI4+IeD+TSwDu43u/np3v6+7v9PdtyX57STPmNvkriQ/WVX7dveN3X31tPz7mQX2Id39t939v6flz0nype7+T929vbvfk+QLSX5hMMI7k7ygqvadbp82Lbt7vo9292e6+67uviqzSH3GCvtZ7peTvGk6S3tLZmE3/7w/1N3/r2c+lln4P32lHa3ghUk+1N1/0d3fT/L7mV1GMX8G/c3dfcP02B9I8oTBvg5McuOCjztyQ3f/0fTz/m6Sd+eesXzqtCyZ/U/E27v7k91953SN9/eSPPVe9v+p6Sz0rVX15mnZi5Kc192f6u7vJfnXSX6qqh4zd7/f7e5bppkAFiaWgd1GVT2oqt5eVV+pqm8l+XiSA6pqr+ks6wuTnJnkxqr6UFUdPd31N5NUkr+aLkf4lWn5IZmdZZz3lSSHrvT4U2RvS3JSVf3DJE/O34VdquopVfU/p8tEvjnNctACT+2QJNcvm2H+eW+qqsumSwhuTfLsBfd7975/sL/prPD1uedz/Nrc999J8uDBvr6e2ZnZ++L6Zbf/R5J9p5/dozML9f82rXt0ktfMxe+tSQ7P7DmNPLG7D5i+XjUtW/4z+Pb0XOZ/BsvnAliIWAZ2J69J8rgkT+nuhyT5J9PySpLuvqS7n5VZ0H0hyZ9My7/W3S/r7kMy+5X+26rqx5PckFmQzTsiyV/fywzvyuyM8kuS/Hl3/83cuncnuSjJ4d390CTn3D3bKm7MLALnZ5g9sdklB+/L7IzwI6cXrV08t99eZd/3eI5VVdNj3dtzHPnvSZ5XVaN/G26f/vuguWX/YNk295h3ivf3ZnZ2+dQkH5x+a5DMAva35+L3gO5+0PQbgB2x/GewX2Znyed/Bqv9HAFWJJaBjfLAqvrRua+9k+yf2XXKt07XG//W3RtX1SOnF7Ltl9mv6r+d5M5p3Qvq714I+I3MwujOzKLzsVV1alXtXVUvzOya1Q/ey1zvyuza3Zdl7hKMyf5Jbunuv62q4zOLv0W8N8mrquqw6UWDZ82t2yfJj2R2Rnt7VW1K8nNz6/8myYFV9dB72fdzquqZVfXAzP6H43tJLl1wtnlvzOy66XdOZ4FTVYdW1Rur6tju3pZZgL54etHir2Sxd8l4d2a/FXhR5s7UZ/Y/O2dOZ52rqvar2Yso99/Bud+d5PSqesL0Px+/k+STd79IEeC+EMvARrk4szC+++v1Sd6U2fW2Nye5LMlH5rZ/QGYheEOSWzK7Vvjl07onJ/lkVX07szO/r+7u67r765m9cO41mf1a/jeTPLe7bx4NNQXWpUn2m/Y17+VJ3lBVtyV5XWahuog/SXJJkk8n+VRmL3q7+/FuS/KqaV/fyCzAL5pb/4XMro2+drpU4R6XKHT3NUlenNmLGm/O7HrsX+juOxacbX5ft2R2rfP3M/t53pbkL5N8M8mWabOXJflXmf08fyILRHl3fzKzs9KHZPbuHncv3zzt7y3Tc9+S8YsP723/f5nk32Z2hv7GzAL+5B3dD8BKqttvpgAAYCXOLAMAwMCqsVxV51XVTVX12cH6qqo3T28Gf9X0vpkAAPBDb5Ezy+dn9mb2I5uSHDV9nZHkj+/7WAAAsPFWjeXu/nhmL6YZOSnJu6Y3078ss/dEva/v0wkAABtuLa5ZPjT3fLP3rRm84T8AAPww2XsN9rHSG/Kv+BYbVXVGZpdqZL/99nvS0UcfvdJmAACwZq644oqbu/vgnbnvWsTy1tzzk6kOy+x9UP+e7j43yblJsrS01Js3b16DhwcAgLGq+srO3nctLsO4KMlp07tiPDXJN7v7xjXYLwAAbKhVzyxX1XuSnJDkoKramtnHzz4wSbr7nMw+hevZmX3y0neSnL6rhgUAgPW0aix39ymrrO8kr1iziQAAYDfhE/wAAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAADAglgEAYEAsAwDAgFgGAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAADAglgEAYEAsAwDAgFgGAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAwsFMtVdWJVXVNVW6rqrBXWP7SqPlBVn66qq6vq9LUfFQAA1teqsVxVeyV5a5JNSY5JckpVHbNss1ck+Vx3H5fkhCR/UFX7rPGsAACwrhY5s3x8ki3dfW1335HkgiQnLdumk+xfVZXkwUluSbJ9TScFAIB1tkgsH5rk+rnbW6dl896S5PFJbkjymSSv7u67lu+oqs6oqs1VtXnbtm07OTIAAKyPRWK5VljWy27/fJIrkxyS5AlJ3lJVD/l7d+o+t7uXunvp4IMP3uFhAQBgPS0Sy1uTHD53+7DMziDPOz3JhT2zJcl1SY5emxEBAGBjLBLLlyc5qqqOnF60d3KSi5Zt89Ukz0ySqnpkkscluXYtBwUAgPW292obdPf2qnplkkuS7JXkvO6+uqrOnNafk+TsJOdX1Wcyu2zjtd198y6cGwAAdrlVYzlJuvviJBcvW3bO3Pc3JPm5tR0NAAA2lk/wAwCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAADAglgEAYEAsAwDAgFgGAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAADAglgEAYEAsAwDAgFgGAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGFgolqvqxKq6pqq2VNVZg21OqKorq+rqqvrY2o4JAADrb+/VNqiqvZK8NcmzkmxNcnlVXdTdn5vb5oAkb0tyYnd/taoesasGBgCA9bLImeXjk2zp7mu7+44kFyQ5adk2pya5sLu/miTdfdPajgkAAOtvkVg+NMn1c7e3TsvmPTbJw6rqo1V1RVWdtlYDAgDARln1MowktcKyXmE/T0ryzCT7JvlEVV3W3V+8x46qzkhyRpIcccQROz4tAACso0XOLG9Ncvjc7cOS3LDCNh/p7tu7++YkH09y3PIddfe53b3U3UsHH3zwzs4MAADrYpFYvjzJUVV1ZFXtk+TkJBct2+bPkjy9qvauqgcleUqSz6/tqAAAsL5WvQyju7dX1SuTXJJkryTndffVVXXmtP6c7v58VX0kyVVJ7kryju7+7K4cHAAAdrXqXn758fpYWlrqzZs3b8hjAwBw/1FVV3T30s7c1yf4AQDAgFgGAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAADAglgEAYEAsAwDAgFgGAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADCwUy1V1YlVdU1Vbquqse9nuyVV1Z1U9f+1GBACAjbFqLFfVXknemmRTkmOSnFJVxwy2+70kl6z1kAAAsBEWObN8fJIt3X1td9+R5IIkJ62w3W8keV+Sm9ZwPgAA2DCLxPKhSa6fu711WvYDVXVokuclOWftRgMAgI21SCzXCst62e03JXltd995rzuqOqOqNlfV5m3bti06IwAAbIi9F9hma5LD524fluSGZdssJbmgqpLkoCTPrqrt3f3++Y26+9wk5ybJ0tLS8uAGAIDdyiKxfHmSo6rqyCR/neTkJKfOb9DdR979fVWdn+SDy0MZAAB+2Kway929vapemdm7XOyV5LzuvrqqzpzWu04ZAIA90iJnltPdFye5eNmyFSO5u19638cCAICN5xP8AABgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAADAglgEAYEAsAwDAgFgGAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAADAglgEAYEAsAwDAgFgGAIABsQwAAANiGQAABsQyAAAMLBTLVXViVV1TVVuq6qwV1r+oqq6avi6tquPWflQAAFhfq8ZyVe2V5K1JNiU5JskpVXXMss2uS/KM7j42ydlJzl3rQQEAYL0tcmb5+CRbuvva7r4jyQVJTprfoLsv7e5vTDcvS3LY2o4JAADrb5FYPjTJ9XO3t07LRn41yYdXWlFVZ1TV5qravG3btsWnBACADbBILNcKy3rFDat+OrNYfu1K67v73O5e6u6lgw8+ePEpAQBgA+y9wDZbkxw+d/uwJDcs36iqjk3yjiSbuvvrazMeAABsnEXOLF+e5KiqOrKq9klycpKL5jeoqiOSXJjkJd39xbUfEwAA1t+qZ5a7e3tVvTLJJUn2SnJed19dVWdO689J8rokByZ5W1UlyfbuXtp1YwMAwK5X3StefrzLLS0t9ebNmzfksQEAuP+oqit29kSuT/ADAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYEMsAADAglgEAYEAsAwDAgFgGAIABsQwAAANiGQAABsQyAAAMiGUAABgQywAAMCCWAQBgQCwDAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGBALAMAwIBYBgCAAbEMAAADYhkAAAbEMgAADIhlAAAYWCiWq+rEqrqmqrZU1VkrrK+qevO0/qqqeuLajwoAAOtr1Viuqr2SvDXJpiTHJDmlqo5ZttmmJEdNX2ck+eM1nhMAANbdImeWj0+ypbuv7e47klyQ5KRl25yU5F09c1mSA6rqUWs8KwAArKtFYvnQJNfP3d46LdvRbQAA4IfK3gtsUyss653YJlV1RmaXaSTJ96rqsws8PvcvByW5eaOHYLfjuGAljgtW4rhgJY/b2TsuEstbkxw+d/uwJDfsxDbp7nOTnJskVbW5u5d2aFr2eI4LVuK4YCWOC1biuGAlVbV5Z++7yGUYlyc5qqqOrKp9kpyc5KJl21yU5LTpXTGemuSb3X3jzg4FAAC7g1XPLHf39qp6ZZJLkuyV5LzuvrqqzpzWn5Pk4iTPTrIlyXeSnL7rRgYAgPWxyGUY6e6LMwvi+WXnzH3fSV6xg4997g5uz/2D44KVOC5YieOClTguWMlOHxc161wAAGA5H3cNAAADuzyWfVQ2K1nguHjRdDxcVVWXVtVxGzEn62u142JuuydX1Z1V9fz1nI+NschxUVUnVNWVVXV1VX1svWdk/S3w78hDq+oDVfXp6bjweqo9XFWdV1U3jd6aeGebc5fGso/KZiULHhfXJXlGdx+b5Oy4Bm2Pt+Bxcfd2v5fZi47Zwy1yXFTVAUneluQXu/snkrxg3QdlXS3498Urknyuu49LckKSP5je1Ys91/lJTryX9TvVnLv6zLKPymYlqx4X3X1pd39junlZZu/dzZ5tkb8vkuQ3krwvyU3rORwbZpHj4tQkF3b3V5Okux0be75FjotOsn9VVZIHJ7klyfb1HZP11N0fz+zPeWSnmnNXx7KPymYlO/pn/qtJPrxLJ2J3sOpxUVWHJnleknPC/cUif188NsnDquqjVXVFVZ22btOxURY5Lt6S5PGZfUjaZ5K8urvvWp/x2E3tVHMu9NZx98GafVQ2e5SF/8yr6qczi+V/vEsnYnewyHHxpiSv7e47ZyeLuB9Y5LjYO8mTkjwzyb5JPlFVl3X3F3f1cGyYRY6Ln09yZZKfSfJjSf6iqv5Xd39rVw/HbmunmnNXx/KafVQ2e5SF/syr6tgk70iyqbu/vk6zsXEWOS6WklwwhfJBSZ5dVdu7+/3rMyIbYNF/R27u7tuT3F5VH09yXBKxvOda5Lg4Pcm/nz4LYktVXZfk6CR/tT4jshvaqebc1Zdh+KhsVrLqcVFVRyS5MMlLnB2631j1uOjuI7v7Md39mCT/NcnLhfIeb5F/R/4sydOrau+qelCSpyT5/DrPyfpa5Lj4ama/bUhVPTLJ45Jcu65TsrvZqebcpWeWfVQ2K1nwuHhdkgOTvG06i7i9u5c2amZ2vQWPC+5nFjkuuvvzVfWRJFcluSvJO7p7xbeOYs+w4N8XZyc5v6o+k9mv31/b3Tdv2NDsclX1nsze+eSgqtqa5LeSPDC5b83pE/wAAGDAJ/gBAMCAWAYAgAGxDAAAA2IZAAAGxDIAAAyIZQAAGBDLAAAwIJYBAGDg/wNXIELh883koAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "alphas = np.logspace(-3, 3, num=50)\n",
    "lasso = Lasso(max_iter=5000)\n",
    "\n",
    "for atm_name in atm_names:\n",
    "    categorical_features_list = ['Weekday', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'Month', 'Day', 'Year']\n",
    "\n",
    "    curr_atm_data = new_data[new_data['ATM Name'] == atm_name].drop('ATM Name', axis=1)\n",
    "    numeric_curr_atm_data = convert_categorical_to_numerical(curr_atm_data, categorical_features_list)\n",
    "\n",
    "    train_data = numeric_curr_atm_data[numeric_curr_atm_data['Year_2017'] == 0]\n",
    "    test_data = numeric_curr_atm_data[numeric_curr_atm_data['Year_2017'] == 1]\n",
    "\n",
    "    X_train = train_data.drop('Total amount Withdrawn', axis=1)\n",
    "    y_train = train_data['Total amount Withdrawn']\n",
    "\n",
    "    X_test = test_data.drop('Total amount Withdrawn', axis=1)\n",
    "    y_test = test_data['Total amount Withdrawn']\n",
    "\n",
    "    train_scores, valid_scores = validation_curve(lasso, X_train, y_train, \"alpha\", \n",
    "                                                  alphas, cv=10, \n",
    "                                                  scoring='neg_root_mean_squared_error',\n",
    "                                                 verbose=2, n_jobs=-1)\n",
    "    \n",
    "    # The scoring used was neg_root_mean_squared_error, so I multiply each element by -1 to get its positive equivalent\n",
    "    train_scores = np.array([-1 * x for x in train_scores])\n",
    "    valid_scores = np.array([-1 * x for x in valid_scores])\n",
    "    \n",
    "    # Taking the mean of the 10 CV errors and converting train_scores and valid_scores to an array of size n_ticks = 50\n",
    "    train_scores_mean = np.mean(train_scores, axis=1) \n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(\"Lasso Validation Curve For {}\".format(atm_name))\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    lw = 2\n",
    "    \n",
    "    # Note semilogx is just plot function where the X-axis is scaled according to logscale\n",
    "    plt.semilogx(alphas, train_scores_mean, label=\"Training score\",\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "    plt.semilogx(alphas, valid_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"navy\", lw=lw)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    # print(train_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Big Street ATM\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 30}\n",
      "\n",
      "Best Score: -114938.39615861795\n",
      "Test RMSE: 275267.13538999046\n",
      "\n",
      "For Mount Road ATM\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 30}\n",
      "\n",
      "Best Score: -182702.97371287917\n",
      "Test RMSE: 323004.6224465292\n",
      "\n",
      "For Airport ATM\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 30}\n",
      "\n",
      "Best Score: -177859.08516294783\n",
      "Test RMSE: 270555.28498196567\n",
      "\n",
      "For KK Nagar ATM\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 30}\n",
      "\n",
      "Best Score: -346205.775131947\n",
      "Test RMSE: 455174.1379231289\n",
      "\n",
      "For Christ College ATM\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 30}\n",
      "\n",
      "Best Score: -188854.2752544846\n",
      "Test RMSE: 562264.9957994054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    4.1s finished\n"
     ]
    }
   ],
   "source": [
    "# lasso_param_grid = {'alpha':[0.005, 0.02, 0.03, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.5, 1]}\n",
    "lasso_param_grid = {'alpha':[3, 3.5, 5, 10, 20, 30]}\n",
    "grid_cv_lasso = GridSearchCV(Lasso(max_iter=2000), lasso_param_grid, verbose=5, n_jobs=-1, scoring='neg_root_mean_squared_error', cv=10)\n",
    "\n",
    "for atm_name in atm_names:\n",
    "    print(\"\\nFor\", atm_name)\n",
    "    model_training_hyperparam_per_atm(atm_name, grid_cv_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> All in all, not very useful. But something's weird, in the best parameter results. For each ATM, either the first alpha value in the list is selected as the best parameter (alpha=0.005 currently) or the last alpha value (alpha=1 currently) so that's definitely something to note </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hyperparameter Tuning for Ridge </h3>\n",
    "<p> Alpha is again the only tunable parameter here, Note that I used mean_squared_error as the scoring parameter here instead of the default R squared metric, but it still didn't make a difference in the weird behaviour I noted above </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Big Street ATM\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 300}\n",
      "\n",
      "Best Score: -28573645730.009846\n",
      "Test RMSE: 167237.2827663761\n",
      "\n",
      "For Mount Road ATM\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 300}\n",
      "\n",
      "Best Score: -59943055536.349144\n",
      "Test RMSE: 324270.4987718039\n",
      "\n",
      "For Airport ATM\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 300}\n",
      "\n",
      "Best Score: -40083222435.99373\n",
      "Test RMSE: 274078.6799585757\n",
      "\n",
      "For KK Nagar ATM\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 300}\n",
      "\n",
      "Best Score: -175963034847.3803\n",
      "Test RMSE: 499684.2321991951\n",
      "\n",
      "For Christ College ATM\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n",
      "Best Parameters:\n",
      " {'alpha': 300}\n",
      "\n",
      "Best Score: -61291171904.104126\n",
      "Test RMSE: 442385.97213670355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# ridge_param_grid = {'alpha':[125, 150, 175, 200, 230, 250,265, 270, 275, 290, 300, 500, 750, 800]}\n",
    "ridge_param_grid = {'alpha':[300, 500, 750, 800, 1000, 1200, 1400, 2000, 4000, 5000, 10000]}\n",
    "grid_cv_ridge = GridSearchCV(Ridge(), ridge_param_grid, verbose=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "for atm_name in atm_names:\n",
    "    print(\"\\nFor\", atm_name)\n",
    "    model_training_hyperparam_per_atm(atm_name, grid_cv_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Big Street ATM, the best CV RMSE was 115241.06043251255 with alpha = 1\n",
      "For Mount Road ATM, the best CV RMSE was 182809.087357492 with alpha = 1\n",
      "For Airport ATM, the best CV RMSE was 178222.93269893242 with alpha = 1\n",
      "For KK Nagar ATM, the best CV RMSE was 346498.30954520905 with alpha = 1\n",
      "For Christ College ATM, the best CV RMSE was 189123.80163207793 with alpha = 1\n"
     ]
    }
   ],
   "source": [
    "# Doing GridSearch manually to verify results\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# alphas = [0.005, 0.02, 0.03, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.5, 1]\n",
    "# # alphas = [0.09, 0.1, 0.15, 0.2, 0.25, 0.5, 1, 1.5, 2]\n",
    "\n",
    "for atm_name in atm_names:\n",
    "    val_errors = []\n",
    "    categorical_features_list = ['Weekday', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'Month', 'Day', 'Year']\n",
    "    \n",
    "    curr_atm_data = new_data[new_data['ATM Name'] == atm_name].drop('ATM Name', axis=1)\n",
    "    numeric_curr_atm_data = convert_categorical_to_numerical(curr_atm_data, categorical_features_list)\n",
    "    \n",
    "    train_data = numeric_curr_atm_data[numeric_curr_atm_data['Year_2017'] == 0]\n",
    "    test_data = numeric_curr_atm_data[numeric_curr_atm_data['Year_2017'] == 1]\n",
    "    \n",
    "    X_train = train_data.drop('Total amount Withdrawn', axis=1)\n",
    "    y_train = train_data['Total amount Withdrawn']\n",
    "\n",
    "    X_test = test_data.drop('Total amount Withdrawn', axis=1)\n",
    "    y_test = test_data['Total amount Withdrawn']\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        lasso = Lasso(alpha=alpha)\n",
    "        # lasso.fit(X_train, y_train)\n",
    "        errors = np.mean(-cross_val_score(estimator=lasso, X=X_train, y=y_train, \n",
    "                                         scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1))\n",
    "        \n",
    "        val_errors.append(errors)\n",
    "\n",
    "#     # print(val_errors)\n",
    "#     best_alpha = alphas[np.argmin(val_errors)]\n",
    "#     best_cv_rmse = min(val_errors)\n",
    "    \n",
    "#     print(\"For {}, the best CV RMSE was {} with alpha = {}\".format(atm_name, best_cv_rmse, best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Really Doubting Hyperparameter Tuning Now </h3>\n",
    "<p> I think I'm doing something wrong at this point, because whenever I add a higher element to the alpha parameter of the tuning grid, GridSearch always selects that value, even when I changed the scoring from the default r2 score to mean_squared_error(), this trend still continues <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Storing Testing Summary as JSON File </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('testing_summary.json', 'w') as file:\n",
    "#     json_string = json.dumps(testing_summary, default=lambda o: o.__dict__, indent=4)\n",
    "#     file.write(json_string)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
